<?xml version="1.0" encoding="utf-8" ?>
<saltlux>
<article>
<action>T</action>
<wms_article>
<art_id>11386708</art_id>
<art_year>2025</art_year>
<art_no>561573</art_no>
<gubun>D</gubun>
<service_daytime>2025-08-06 16:02:31</service_daytime>
<title><![CDATA[누구나 AI로 성과내는 시대 … AI 오류 분별할 '명석한 전문가' 키워야]]></title>
<sub_title><![CDATA[]]></sub_title>
<media_code>01</media_code>
<writers><![CDATA[]]></writers>
<free_type>F</free_type>
<pub_div>P</pub_div>
<pub_date>20250807</pub_date>
<pub_edition>11</pub_edition>
<pub_section>02</pub_section>
<pub_page>4</pub_page>
<reg_dt>2025-08-06 16:02:31</reg_dt>
<mod_dt></mod_dt>
<art_org_class>MK300111</art_org_class>
</wms_article>
<wms_article_body>
<body><![CDATA[<img src='https://wimg.mk.co.kr/news/cms/202508/07/20250807_01110204000002_M00.jpg' alt='키론 라빈드란'>

인공지능(AI)의 가장 큰 위험성은 인간 일자리 대체로 알려져 있다. 하지만 필자는 이보다 더 큰 리스크를 제안한다. 바로 유능해 보이지만 실제로는 판단력이 부족한 전문가의 탄생이다.
AI도구 사용으로 전문가들의 사고능력을 키웠던 업무가 자동화됐다. 이에 따라 현 세대 전문가들은 훌륭한 결과물을 내놓지만 어떻게 결과물을 도출했는지 설명하는 능력이 부족하다.
오늘날 AI를 사용하는 전문가들은 조직에서 사실상 유능한 사람으로 여겨진다. 하지만 업무 환경·상황이 바뀐다면 해당 직원들은 그로부터 오는 압박감에 굴복하거나, 윤리성을 간과하거나, 얼어붙을 수 있다. AI가 제공하는 답변이 충분하지 않을 때 상황에 맞는 대응 방식을 제안할 수 있는 사람이 조직에 필요하다. 결국 AI가 제공하는 해답에는 한계가 있다.
AI 도움을 받는 업무가 위험성을 유발하는 이유를 이해하기 위해서는 두 가지 중요한 측면을 고려해야 한다. 하나는 AI 사용자의 지식, 또 다른 하나는 수행해야 하는 업무의 특성이다.
초보 전문가부터 노련한 전문가까지 개인 전문성 범위는 넓다. 그리고 전문가의 업무는 두 가지 유형으로 나뉜다. 하나는 개인 지식을 표현하고 이를 확인할 수 있는 업무다. 다른 하나는 경험을 통해서만 얻을 수 있는 암묵적 지식이 필요한 일이다.
개인 전문성 수준과 업무 유형에 따라 AI 사용방식은 다르다. 전문성이 높은 사람이 암묵적 지식이 요구되는 일을 하는 상황의 예는 숙련된 의사가 진단을 위해 AI를 사용하는 것이다. 반대로 암묵적 지식이 요구되는 업무를 전문성이 낮은 직원이 AI를 사용해 진행하는 사례로는 주니어급 애널리스트가 AI 도움을 받아 자신이 모르는 시장의 역학을 파악하는 것이다. 전문성이 없는 상태로 AI를 사용한다면 무엇인가 잘못되더라도 눈치챌 수 없다.
혹자는 AI로 인해 인간의 판단력과 전문성이 떨어지는 것이 사람들의 무능력함이 아닌, 인지 진화(cognitive evolution)라고 주장할 수도 있다. 앤디 클라크 서섹스대 교수는 생성형 AI가 '확장된 사고(extended minds)'의 일부라고 제안한다. 계산기, 검색 엔진 등 인류는 오랫동안 도구를 사용해 사고방식을 강화해왔다. AI가 이런 도구들과 다른 것이 무엇일까?
차이점은 오류에 빠질 때 드러난다. 계산기가 고장나면 나타나는 오류는 명백하다. 하지만 개인이 이해하지 못하는 분야에 대한 내용을 챗GPT가 그럴듯하게 분석해 내놓는다면, 내용이 틀리더라도 해당 오류는 인식되지 않는다. 클라크 교수는 AI가 출력하는 결과물을 판단하기 위해 인간에게 새로운 '메타인지 능력'이 필요하다고 말한다. 하지만 전문가가 아닌 사람들에게 부족한 것이 바로 해당 능력이다. 사람들은 본인이 모르는 것에 대한 판단력을 키울 수 없다. 차세대 지식 노동자들이 처음부터 AI를 사용해 전략, 분석, 문제해결 능력을 키우려고 한다면 어떤 일이 벌어질까? 해당 '전문가'들은 세련된 결과물을 내놓겠지만 '진짜 통찰력'과 '설득력 있어 보이지만 사실은 허튼소리'인 말을 구분할 수 있는 판단력이 부족할 것이다.
AI가 불러올 수 있는 이러한 상황들에 대한 해결책은 결국 학습에 있다. 구체적으로 세 가지 차원의 학습이 필요하다. 첫 번째는 개인 학습이다. 예를 들어 챗GPT를 사용해 주어진 케이스에 맞는 해결책을 찾기 전 경영대 학생은 스스로 해당 케이스의 주요 정보를 먼저 파악해야 한다. 두 번째는 조직 학습이다. 기업은 주니어급 사원들을 해고하거나 해당 일자리를 없애지 말아야 한다. 현재 주니어급 직원들이 가까운 미래의 전문가들이다. AI가 잘못된 결과물을 낼 때 이들이 오류를 알아챈다. 이를 알고 인턴십 프로그램을 키운 기업의 예는 TSMC다. 마지막은 시스템 차원의 학습이다. AI어시스턴트 활용을 드러나게 해서 AI 생성물을 검토하는 학습이다.
[키론 라빈드란 IE 비즈니스 스쿨 교수]]]></body>
</wms_article_body>
<wms_article_summary>
<summary><![CDATA[인공지능(AI)의 가장 큰 위험성은 인간 일자리 대체로 알려져 있다. 하지만 필자는 이보다 더 큰 리스크를 제안한다. 바로 유능해 보이지만 실제로는 판단력이 부족한 전문가의 탄생이다.
AI도구 사용으로 전문가들의 사고능력을 키웠던 업무가 자동화됐다. 이에 따라 현 세대 전문가들은 훌륭한 결과물을 내놓지만 어떻게 결과물을 도출했는지 설명하는 능력이 부족하다.
오늘날 AI를 사용하는 전문가들은 조직에서 사실상 유능한 사람으로 여겨진다. 하지만 업무 환경·상]]></summary>
</wms_article_summary>
<stock_codes>
</stock_codes>
<wms_article_keywords>
</wms_article_keywords>
<wms_code_classes>
<wms_code_class>
<code_id>MK300111</code_id>
<code_nm><![CDATA[MBA]]></code_nm>
<large_code_id>0</large_code_id>
<large_code_nm><![CDATA[뉴스]]></large_code_nm>
<middle_code_id>00501</middle_code_id>
<middle_code_nm><![CDATA[기업]]></middle_code_nm>
<small_code_id>MK300111</small_code_id>
<small_code_nm><![CDATA[MBA]]></small_code_nm>
</wms_code_class>
</wms_code_classes>
<wms_article_images>
<wms_article_image>
<image_url><![CDATA[https://wimg.mk.co.kr/news/cms/202508/07/20250807_01110204000002_M00.jpg]]></image_url>
<image_caption><![CDATA[키론 라빈드란]]></image_caption>
</wms_article_image>
</wms_article_images>
<article_url><![CDATA[https://www.mk.co.kr/article/11386708]]></article_url>
</article>
</saltlux>
